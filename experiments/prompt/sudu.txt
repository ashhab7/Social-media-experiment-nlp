(social) bash-4.4$python run_train_llama3_alternate.py 
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:31<00:00, 15.51s/it]
Error in text generation: CUDA driver initialization failed, you might not have a CUDA gpu.
(social) bash-4.4$nvidia-smi
Mon Dec 16 15:07:34 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.54.14              Driver Version: 550.54.14      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:07:00.0 Off |                    0 |
| N/A   28C    P0             57W /  400W |     263MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA A100-SXM4-80GB          Off |   00000000:47:00.0 Off |                    0 |
| N/A   29C    P0             59W /  400W |     115MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     22217      G   /usr/libexec/Xorg                             108MiB |
|    0   N/A  N/A     22334      G   /usr/bin/gnome-shell                          135MiB |
|    1   N/A  N/A     22217      G   /usr/libexec/Xorg                             106MiB |
+-----------------------------------------------------------------------------------------+
(social) bash-4.4$module load cuda

The following have been reloaded with a version change:
  1) cuda/11.8.0 => cuda/12.4.1

(social) bash-4.4$module load cudnn

The following have been reloaded with a version change:
  1) cuda/12.4.1 => cuda/11.8.0

(social) bash-4.4$python run_train_llama3_alternate.py 
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:12<00:00,  6.43s/it]
Error in text generation: CUDA driver initialization failed, you might not have a CUDA gpu.
(social) bash-4.4$
(social) bash-4.4$python run_train_llama3_alternate.py 
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:18<00:00,  9.06s/it]
Error in text generation: CUDA driver initialization failed, you might not have a CUDA gpu.
(social) bash-4.4$python run_train_llama3_alternate.py 
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:07<00:00,  3.82s/it]
Error in text generation: Expected one of cpu, cuda, ipu, xpu, mkldnn, opengl, opencl, ideep, hip, ve, fpga, maia, xla, lazy, vulkan, mps, meta, hpu, mtia, privateuseone device type at start of device string: auto
(social) bash-4.4$
(social) bash-4.4$
(social) bash-4.4$python run_train_llama3_alternate.py 
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:08<00:00,  4.18s/it]
/home/ftm2nu/.conda/envs/social/lib/python3.12/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA driver initialization failed, you might not have a CUDA gpu. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)
  return torch._C._cuda_getDeviceCount() > 0
^CTraceback (most recent call last):
  File "/sfs/weka/scratch/ftm2nu/sentiment_analysis/run_train_llama3_alternate.py", line 162, in <module>
    main()
  File "/sfs/weka/scratch/ftm2nu/sentiment_analysis/run_train_llama3_alternate.py", line 140, in main
    # Generate text
              ^^^^^
  File "/sfs/weka/scratch/ftm2nu/sentiment_analysis/run_train_llama3_alternate.py", line 65, in load_and_generate_text
    full_prompt = f"<s>[INST] {prompt} [/INST]"
             ^^^^^^^^^^^^^^^^^
  File "/home/ftm2nu/.conda/envs/social/lib/python3.12/site-packages/transformers/pipelines/text_generation.py", line 272, in __call__
    return super().__call__(text_inputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ftm2nu/.conda/envs/social/lib/python3.12/site-packages/transformers/pipelines/base.py", line 1302, in __call__
    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ftm2nu/.conda/envs/social/lib/python3.12/site-packages/transformers/pipelines/base.py", line 1309, in run_single
    model_outputs = self.forward(model_inputs, **forward_params)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ftm2nu/.conda/envs/social/lib/python3.12/site-packages/transformers/pipelines/base.py", line 1209, in forward
    model_outputs = self._forward(model_inputs, **forward_params)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ftm2nu/.conda/envs/social/lib/python3.12/site-packages/transformers/pipelines/text_generation.py", line 370, in _forward
    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ftm2nu/.conda/envs/social/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ftm2nu/.conda/envs/social/lib/python3.12/site-packages/transformers/generation/utils.py", line 2215, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/home/ftm2nu/.conda/envs/social/lib/python3.12/site-packages/transformers/generation/utils.py", line 3206, in _sample
    outputs = self(**model_inputs, return_dict=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ftm2nu/.conda/envs/social/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ftm2nu/.conda/envs/social/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ftm2nu/.conda/envs/social/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py", line 1190, in forward
    outputs = self.model(
              ^^^^^^^^^^^
  File "/home/ftm2nu/.conda/envs/social/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ftm2nu/.conda/envs/social/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ftm2nu/.conda/envs/social/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py", line 945, in forward
    layer_outputs = decoder_layer(
                    ^^^^^^^^^^^^^^
  File "/home/ftm2nu/.conda/envs/social/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ftm2nu/.conda/envs/social/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ftm2nu/.conda/envs/social/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py", line 692, in forward
    hidden_states = self.mlp(hidden_states)
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ftm2nu/.conda/envs/social/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ftm2nu/.conda/envs/social/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ftm2nu/.conda/envs/social/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py", line 258, in forward
    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
                                                                ^^^^^^^^^^^^^^^
  File "/home/ftm2nu/.conda/envs/social/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ftm2nu/.conda/envs/social/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ftm2nu/.conda/envs/social/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
^C
(social) bash-4.4$^C
(social) bash-4.4$^C
(social) bash-4.4$^C
^C
(social) bash-4.4$
(social) bash-4.4$
(social) bash-4.4$python run_train_llama3_alternate.py 
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:08<00:00,  4.15s/it]
Error in text generation: CUDA driver initialization failed, you might not have a CUDA gpu.
(social) bash-4.4$nvidia-smi
Mon Dec 16 15:13:48 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.54.14              Driver Version: 550.54.14      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:07:00.0 Off |                    0 |
| N/A   28C    P0             57W /  400W |     264MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA A100-SXM4-80GB          Off |   00000000:47:00.0 Off |                    0 |
| N/A   29C    P0             59W /  400W |     115MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     22217      G   /usr/libexec/Xorg                             108MiB |
|    0   N/A  N/A     22334      G   /usr/bin/gnome-shell                          136MiB |
|    1   N/A  N/A     22217      G   /usr/libexec/Xorg                             106MiB |
+-----------------------------------------------------------------------------------------+
(social) bash-4.4$sudo nvidia-smi -mig 0

We trust you have received the usual lecture from the local System
Administrator. It usually boils down to these three things:

    #1) Respect the privacy of others.
    #2) Think before you type.
    #3) With great power comes great responsibility.

[sudo] password for ftm2nu: 
Sorry, try again.
[sudo] password for ftm2nu: 
ftm2nu is not in the sudoers file.  This incident will be reported.
(social) bash-4.4$